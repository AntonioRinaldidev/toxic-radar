**Intelligent toxicity detection, analysis, and mitigation system using transformer-based models with universal system optimization.**

## ğŸ¯ Overview

The AI section provides a complete pipeline for processing toxic language through:
- **Classification**: Advanced toxicity detection using Detoxify
- **Reasoning**: Rule-based logical refinement of toxicity scores  
- **Paraphrasing**: Intelligent text rewriting with multi-criteria ranking
- **Analysis**: Full pipeline orchestration

**Key Features:**
- ğŸŒ **Universal Compatibility**: Automatically optimizes for any system (Windows/Linux/macOS, CPU/GPU)
- âš¡ **Auto-Adaptive Performance**: Adjusts parameters based on hardware capabilities
- ğŸ—ï¸ **Microservices Architecture**: Scalable, independent services
- ğŸ§  **Advanced Reasoning**: Rule-based toxicity score refinement
- ğŸ¯ **Multi-Criteria Ranking**: Toxicity + Similarity + Fluency optimization

## ğŸš€ Quick Start

### 1. Universal Setup
```bash
# Clone and navigate to project
git clone <your-repo>
cd ToxicRadar

# Run universal setup (works on any system)
python3 -m venv venv
source venv/bin/activate
chmod +x AI/universal_startup.sh
./AI/universal_startup.sh
```

The setup script automatically:
- Detects your OS and hardware
- Creates optimized virtual environment
- Installs dependencies (PyTorch optimized for your system)
- Tests installation
- Creates project structure

### 2. Start Services

**Individual Services:**
```bash
# Classification Service (Port 8001)
python -m uvicorn AI.classifier.classifier:app --host 0.0.0.0 --port 8001

# Reasoning Service (Port 8002)  
python -m uvicorn AI.reasoning.reasoning:app --host 0.0.0.0 --port 8002

# Paraphrasing Service (Port 8000)
python -m uvicorn AI.paraphraser.service:app --host 0.0.0.0 --port 8000

# Analysis Service (Port 8003)
python -m uvicorn AI.analyze.service:app --host 0.0.0.0 --port 8003
```

**Main Service (Recommended):**
```bash
# Starts the complete paraphrasing service
python -m uvicorn AI.paraphraser.service:app --host 0.0.0.0 --port 8000
```

### 3. Test the System

```bash
# Test single text paraphrasing
curl -X POST "http://localhost:8000/paraphrase" \
  -H "Content-Type: application/json" \
  -d '{"text": "You are such an idiot!", "num_candidates": 3}'

# Check system health
curl "http://localhost:8000/health"
```

## ğŸ—ï¸ Architecture & Flow

### System Flow Diagram

```mermaid
flowchart TD
    Start([ğŸš€ universal_startup.sh]) --> SystemDetect[ğŸ” System Detection<br/>OS, GPU, Memory Analysis]
    SystemDetect --> VenvSetup[ğŸ“¦ Environment Setup<br/>Virtual env + Dependencies]
    VenvSetup --> CoreInit[ğŸ§  Core Initialization<br/>Hardware optimization]
    CoreInit --> ModelLoad[ğŸ¤– Model Loading<br/>T5 + Detoxify models]
    ModelLoad --> Services{ğŸš€ Services}
    
    Services --> Classifier[ğŸ“Š Classification Service<br/>Toxicity detection]
    Services --> Reasoning[ğŸ§® Reasoning Service<br/>Rule-based refinement]
    Services --> Paraphrasing[ğŸ“ Paraphrasing Service<br/>Text generation + ranking]
    Services --> Analysis[ğŸ”„ Analysis Service<br/>Pipeline orchestrator]
    
    Paraphrasing --> UserRequest[ğŸ‘¤ User Request]
    UserRequest --> Generation[ğŸ¯ Text Generation]
    Generation --> Scoring[ğŸ” Multi-Criteria Scoring]
    Scoring --> Ranking[ğŸ† Candidate Ranking]
    Ranking --> Response[âœ¨ Final Response]
```

### Processing Pipeline

For each text input, the system:

1. **ğŸ” Classification**: Detoxify model detects toxicity levels
2. **ğŸ§® Reasoning**: Rule engine applies logical constraints
3. **ğŸ¯ Generation**: T5 model creates multiple paraphrases  
4. **ğŸ“Š Scoring**: Each candidate scored for:
   - **Toxicity**: Using Detoxify + reasoning rules
   - **Similarity**: Semantic similarity to original (Sentence-BERT)
   - **Fluency**: Language quality (T5 perplexity)
5. **ğŸ† Ranking**: Weighted utility score: `(1-toxicity)Ã—0.5 + similarityÃ—0.3 + fluencyÃ—0.2`
6. **âœ¨ Response**: Best candidates returned with detailed metadata

## ğŸ“ Project Structure

```
AI/
â”œâ”€â”€ universal_startup.sh          # Universal setup script
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ system_detector.py        # Hardware detection & optimization
â”œâ”€â”€ classifier/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classifier.py             # Toxicity classification service
â”‚   â””â”€â”€ service.py                # Test client
â”œâ”€â”€ reasoning/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ reasoning.py              # Reasoning service
â”‚   â””â”€â”€ logic.py                  # Rule engine
â”œâ”€â”€ paraphraser/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ service.py                # Main paraphrasing service
â”‚   â”œâ”€â”€ shared_model.py           # Universal model manager
â”‚   â”œâ”€â”€ generator.py              # Text generation
â”‚   â”œâ”€â”€ scorer.py                 # Multi-criteria scoring
â”‚   â””â”€â”€ voting.py                 # Ranking algorithms
â””â”€â”€ analyze/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ service.py                # Analysis orchestrator
```

## ğŸ”§ Services & APIs

### 1. Classification Service (`classifier.py`)
**Port**: 8001

```python
POST /classify              # Single text classification
POST /classify_batch        # Batch processing  
GET  /health               # Service status
GET  /info                 # Service information
```

**Example**:
```bash
curl -X POST "http://localhost:8001/classify" \
  -H "Content-Type: application/json" \
  -d '{"text": "You are terrible!"}'
```

### 2. Reasoning Service (`reasoning.py`)
**Port**: 8002

```python
POST /reason               # Apply logical rules to toxicity scores
```

**Rule Examples**:
- `identity_attack > 0.1` â†’ Force `toxicity = 0.9`
- `severe_toxicity > 0.3` â†’ Ensure `toxicity â‰¥ severe_toxicity`
- `threat > 0.3` â†’ Set `toxicity = 0.8`

### 3. Paraphrasing Service (`service.py`) 
**Port**: 8000 (Main Service)

```python
POST /paraphrase           # Single text paraphrasing
POST /paraphrase_batch     # Batch processing
POST /paraphrase_advanced  # Advanced controls
GET  /health              # System status
GET  /system_info         # Comprehensive system info
GET  /capabilities        # System capabilities
```

**Generation Modes**:
- `auto`: Automatically selects best mode for your system
- `efficient`: Memory-efficient for resource-constrained systems
- `quality`: High-quality for powerful systems  
- `universal`: Balanced approach

### 4. Analysis Service (`service.py`)
**Port**: 8003

```python
POST /analyze             # Full pipeline: Classification + Reasoning
```

## âš™ï¸ System Optimizations

### Automatic Hardware Detection

The system automatically detects and optimizes for:

**CUDA (NVIDIA GPU)**:
- Float16 precision for memory efficiency
- GPU memory management and batch optimization
- CUDA autocast for performance

**MPS (Apple Silicon)**:
- Metal Performance Shaders optimization
- Float32 for stability
- Shared memory management

**CPU**:
- Multi-threading optimization
- Conservative memory usage
- Efficient batch processing

### Performance Adaptations

| System Type | Batch Size | Workers | Max Candidates | Memory Mode |
|-------------|------------|---------|----------------|-------------|
| High-end GPU (â‰¥12GB) | 16 | 8 | 20 | Float16 |
| Mid-range GPU (8-12GB) | 8 | 6 | 15 | Float16/32 |
| Low-end GPU (<8GB) | 4 | 4 | 10 | 8-bit/4-bit |
| High-end CPU (â‰¥32GB RAM) | 8 | 8 | 15 | Float32 |
| Standard CPU (16-32GB) | 4 | 4 | 10 | Float32 |
| Low-end CPU (<16GB) | 2 | 2 | 6 | Float32 |

## ğŸ§  Advanced Features

### Rule-Based Reasoning Engine

The reasoning system applies logical constraints to ensure consistency:

```python
# Example rules from logic.py
def identity_attack_rule():
    return {
        "condition": lambda labels: labels["identity_attack"] > 0.1 and labels["toxicity"] < 0.5,
        "action": lambda labels: labels.update({"toxicity": 0.9}),
        "explanation": "identity_attack > 0.1 â†’ forced toxicity = 0.9"
    }
```

### Multi-Criteria Ranking

Candidates are ranked using a weighted utility function:

```python
utility_score = (1 - toxicity) * 0.5 + similarity * 0.3 + fluency * 0.2
```

Alternative ranking methods available:
- **Weighted Voting**: Multiple voting strategies
- **Adaptive Scoring**: Context-dependent weights
- **Borda Count**: Rank-based voting
- **Copeland Method**: Pairwise comparisons

### Memory Management

- **Adaptive Caching**: LRU caches sized based on available RAM
- **Model Sharing**: Single model instance across services
- **Automatic Cleanup**: Device-specific memory clearing
- **Resource Monitoring**: Real-time usage tracking

## ğŸ“Š Monitoring & Health

### Health Endpoints

```bash
# Paraphrasing service health
curl "http://localhost:8000/health"

# Comprehensive system info
curl "http://localhost:8000/system_info"

# System capabilities
curl "http://localhost:8000/capabilities"
```

### Performance Metrics

The system provides real-time monitoring of:
- Processing times
- Memory usage (CPU/GPU)
- Model loading status
- Toxicity reduction effectiveness
- Batch processing efficiency
- System resource utilization

## ğŸ”¬ Development & Testing

### Running Tests

```bash
# Install development dependencies
pip install pytest pytest-asyncio

# Run tests
pytest AI/tests/
```

### Example Usage

```python
import requests

# Single text paraphrasing
response = requests.post("http://localhost:8000/paraphrase", json={
    "text": "You're such an idiot!",
    "num_candidates": 3,
    "mode": "auto"
})

result = response.json()
print(f"Best paraphrase: {result['candidates'][0]['text']}")
print(f"Toxicity reduction: {result['metadata']['toxicity_reduction']}")
```

### Batch Processing

```python
# Batch processing
response = requests.post("http://localhost:8000/paraphrase_batch", json={
    "texts": ["You're terrible!", "This is awful!", "I hate this!"],
    "num_candidates_each": 3,
    "mode": "auto"
})

results = response.json()
for i, result in enumerate(results['results']):
    print(f"Text {i+1}: {result['candidates'][0]['text']}")
```

## ğŸ› Troubleshooting

### Common Issues

**Memory Errors**:
- The system automatically adjusts to available memory
- Try `mode="efficient"` for resource-constrained systems
- Check `/health` endpoint for memory usage

**Slow Performance**:
- System optimizes automatically based on hardware
- First request may take 30-60s for model loading
- Subsequent requests are much faster

**Installation Issues**:
- Run `./universal_startup.sh` again
- Check Python version (3.8+ required)
- Ensure virtual environment is activated

### Logs

```bash
# View service logs
tail -f logs/paraphraser.log
tail -f logs/classifier.log
```

## ğŸ“ˆ Performance Benchmarks

Typical performance on different systems:

| System | First Request | Subsequent Requests | Batch (10 texts) |
|--------|---------------|-------------------|------------------|
| RTX 4090 | 2-5s | 0.5-1s | 3-6s |
| RTX 3080 | 3-8s | 1-2s | 5-10s |
| M1 Mac | 5-10s | 2-3s | 8-15s |
| CPU (16GB) | 10-30s | 3-6s | 15-45s |

## ğŸ“ License

[Your License Here]

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Make changes
4. Run tests
5. Submit pull request

## ğŸ“ Support

- Email: contact@antoniorinaldidev.com
